{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451b557c",
   "metadata": {},
   "source": [
    "# Palm Print Recognition with Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2047582",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be021590",
   "metadata": {},
   "source": [
    "## Setup Environment and GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef214c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment and GPU Configuration\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU(s) available: {gpus}\")\n",
    "        print(\"GPU memory growth set.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error setting up GPU: {e}\")\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee41d9",
   "metadata": {},
   "source": [
    "## Define Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters for model training\n",
    "EPOCHS_FEATURE_EXTRACTION = 20\n",
    "EPOCHS_FINE_TUNE = 10\n",
    "LEARNING_RATE_INITIAL = 0.001\n",
    "LEARNING_RATE_FINE_TUNE = 1e-5\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"- Feature extraction epochs: {EPOCHS_FEATURE_EXTRACTION}\")\n",
    "print(f\"- Fine-tuning epochs: {EPOCHS_FINE_TUNE}\")\n",
    "print(f\"- Image size: {IMG_SIZE}\")\n",
    "print(f\"- Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc21694",
   "metadata": {},
   "source": [
    "## Palm ROI Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_palm_roi(image, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Extracts a square Region of Interest (ROI) from the center of the palm.\n",
    "    This version has the corrected thresholding logic.\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "    \n",
    "    # ### THE FIX IS HERE: ###\n",
    "    # We remove THRESH_BINARY_INV to correctly segment the bright palm on the dark background.\n",
    "    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contours:\n",
    "        return None\n",
    "\n",
    "    hand_contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    if cv2.contourArea(hand_contour) < 5000: # Filter out small noise\n",
    "        return None\n",
    "\n",
    "    M = cv2.moments(hand_contour)\n",
    "    if M[\"m00\"] == 0:\n",
    "        return None\n",
    "    cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "    # Crop a fixed-size square around the center of the palm\n",
    "    CROP_SIZE = 200 \n",
    "    half_crop = CROP_SIZE // 2\n",
    "    \n",
    "    y1 = max(0, cy - half_crop)\n",
    "    y2 = min(image.shape[0], cy + half_crop)\n",
    "    x1 = max(0, cx - half_crop)\n",
    "    x2 = min(image.shape[1], cx + half_crop)\n",
    "\n",
    "    roi = image[y1:y2, x1:x2]\n",
    "    \n",
    "    if roi.size == 0:\n",
    "        return None\n",
    "    \n",
    "    return cv2.resize(roi, size, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d9fa5",
   "metadata": {},
   "source": [
    "## Dataset Exploration & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "DATASET_DIR = 'datasets/Sapienza University Mobile Palmprint Database(SMPD)'\n",
    "data_dir = pathlib.Path(DATASET_DIR)\n",
    "print(f\"Dataset path set to: {data_dir}\")\n",
    "\n",
    "if not data_dir.exists():\n",
    "    print(f\"Error: Dataset directory not found at '{DATASET_DIR}'\")\n",
    "else:\n",
    "    print(\"Dataset directory found!\")\n",
    "    \n",
    "    # Correctly find and count all images\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "    image_paths = [f for f in data_dir.rglob('*') if f.suffix.lower() in valid_extensions]\n",
    "    image_count = len(image_paths)\n",
    "    print(f\"Total images found: {image_count}\")\n",
    "\n",
    "    class_names_list = sorted([item.name for item in data_dir.glob('*') if item.is_dir()])\n",
    "    num_classes = len(class_names_list)\n",
    "    print(f\"Total classes (individuals): {num_classes}\")\n",
    "\n",
    "    print(\"\\\\nImage count per class (sample):\")\n",
    "    for class_name in class_names_list[:10]:\n",
    "        class_dir = data_dir / class_name\n",
    "        count = len([f for f in class_dir.rglob('*') if f.suffix.lower() in valid_extensions])\n",
    "        print(f\"- ID {class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677658f7",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72fe03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading and preprocessing dataset with ROI extraction...\")\n",
    "images = []\n",
    "labels = []\n",
    "print(f\"Processing {len(class_names_list)} classes...\")\n",
    "\n",
    "valid_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "\n",
    "for class_idx, class_name in enumerate(class_names_list):\n",
    "    class_dir = data_dir / class_name\n",
    "    \n",
    "    class_images = [f for f in class_dir.rglob('*') if f.suffix.lower() in valid_extensions]\n",
    "    \n",
    "    print(f\"Processing class {class_name}: Found {len(class_images)} images\", end=' -> ')\n",
    "    \n",
    "    processed_count = 0\n",
    "    for img_path in class_images:\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is not None:\n",
    "            roi = extract_palm_roi(img, size=IMG_SIZE)\n",
    "            if roi is not None:\n",
    "                images.append(roi)\n",
    "                labels.append(class_idx)\n",
    "                processed_count += 1\n",
    "    print(f\"Successfully processed {processed_count} ROIs.\")\n",
    "\n",
    "X = np.array(images)\n",
    "y = np.array(labels)\n",
    "class_names = class_names_list\n",
    "\n",
    "print(f\"\\\\nDataset loaded successfully!\")\n",
    "print(f\"- Total processed images: {len(X)}\")\n",
    "print(f\"- Image shape: {X.shape}\")\n",
    "print(f\"- Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e3040",
   "metadata": {},
   "source": [
    "## Data Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample processed images\n",
    "print(\"--- Visualizing Sample Processed ROI Data ---\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Display first 12 images\n",
    "for i in range(min(12, len(X))):\n",
    "    ax = plt.subplot(3, 4, i + 1)\n",
    "    \n",
    "    # Convert to displayable format\n",
    "    img = X[i].astype(\"uint8\")\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Get class name from label\n",
    "    label_idx = y[i]\n",
    "    class_name = class_names[label_idx]\n",
    "    plt.title(f\"ID: {class_name}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Sample Palm ROI Images from Dataset\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show class distribution analysis\n",
    "print(\"\\n--- Class Distribution Analysis ---\")\n",
    "unique_labels, counts = np.unique(y, return_counts=True)\n",
    "print(f\"Images per class - Min: {counts.min()}, Max: {counts.max()}, Mean: {counts.mean():.1f}\")\n",
    "\n",
    "# Plot class distribution (for first 20 classes if there are many)\n",
    "plt.figure(figsize=(12, 6))\n",
    "classes_to_show = min(20, len(unique_labels))\n",
    "plt.bar(range(classes_to_show), counts[:classes_to_show])\n",
    "plt.title(f\"Images per Class (First {classes_to_show} classes)\")\n",
    "plt.xlabel(\"Class Index\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xticks(range(classes_to_show), [class_names[i] for i in range(classes_to_show)], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6ef6b",
   "metadata": {},
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"- Training samples: {len(X_train)}\")\n",
    "print(f\"- Validation samples: {len(X_val)}\")\n",
    "print(f\"- Training shape: {X_train.shape}\")\n",
    "print(f\"- Validation shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f469234",
   "metadata": {},
   "source": [
    "## Create TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05baa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets with optimization\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_ds = val_ds.batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"TensorFlow datasets created:\")\n",
    "print(f\"- Training batches: {len(train_ds)}\")\n",
    "print(f\"- Validation batches: {len(val_ds)}\")\n",
    "print(f\"- Batch size: {BATCH_SIZE}\")\n",
    "print(f\"- Image size: {IMG_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ee9f2",
   "metadata": {},
   "source": [
    "## Data Augmentation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15825b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation layers for training\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.2),\n",
    "], name='augmentation')\n",
    "\n",
    "print(\"--- Preview of Data Augmentation ---\")\n",
    "\n",
    "# Take one batch and show original vs augmented versions\n",
    "for images, labels in train_ds.take(1):\n",
    "    sample_image = images[0:1]  # Take first image\n",
    "    sample_label = labels[0]\n",
    "    \n",
    "    plt.figure(figsize=(15, 3))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.imshow(sample_image[0].numpy().astype(\"uint8\"))\n",
    "    plt.title(f\"Original\\nID: {class_names[sample_label]}\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Show 4 augmented versions\n",
    "    for i in range(4):\n",
    "        augmented = data_augmentation(sample_image)\n",
    "        plt.subplot(1, 5, i + 2)\n",
    "        plt.imshow(augmented[0].numpy().astype(\"uint8\"))\n",
    "        plt.title(f\"Augmented {i+1}\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(\"Data Augmentation Preview\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "print(\"Data augmentation setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb47b2",
   "metadata": {},
   "source": [
    "## Build and Compile Base Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Building and Compiling Full Model Architecture ---\")\n",
    "\n",
    "# --- 1. Define Input Shape and Augmentation ---\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.2)\n",
    "], name='augmentation')\n",
    "\n",
    "# --- 2. Load the Pre-Trained Base Model ---\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=IMG_SHAPE,\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "# We set the name and freeze it for initial training\n",
    "base_model._name = 'mobilenetv2_base'\n",
    "base_model.trainable = False\n",
    "\n",
    "# --- 3. Build the Full Model by Chaining Layers ---\n",
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "# Correct Order: Augment on [0-255] pixels, then Rescale to [0-1]\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = base_model(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "# This line creates the model variable that was missing before\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# --- 4. Compile the Model ---\n",
    "print(\"\\\\n--- Compiling the Model for Training ---\")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_INITIAL),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# --- 5. Display Summary and Configure Callbacks ---\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_palm_model.keras', \n",
    "        save_best_only=True, \n",
    "        monitor='val_accuracy', \n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5, \n",
    "        monitor='val_loss', \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\\\nModel is built, compiled, and ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ed50a",
   "metadata": {},
   "source": [
    "## Compile Model for Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4f47e",
   "metadata": {},
   "source": [
    "## Phase 1 Training - Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931582dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 Training - Feature Extraction\n",
    "print(\"--- Starting Training Phase 1: Feature Extraction ---\")\n",
    "print(\"Base model is frozen, training only the classifier head...\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS_FEATURE_EXTRACTION,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"--- Phase 1 Training Complete ---\")\n",
    "print(f\"Completed {len(history.history['accuracy'])} epochs\")\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e604e668",
   "metadata": {},
   "source": [
    "## Model Fine-tuning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5585b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Fine-tuning Setup\n",
    "print(\"--- Preparing for Fine-Tuning ---\")\n",
    "\n",
    "# Unfreeze base model for fine-tuning\n",
    "base_model = model.get_layer('mobilenetv2_base')\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze early layers, only fine-tune top layers\n",
    "fine_tune_at = 100\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"Fine-tuning from layer {fine_tune_at} onwards...\")\n",
    "print(f\"Total layers in base model: {len(base_model.layers)}\")\n",
    "print(f\"Trainable layers: {len([layer for layer in base_model.layers if layer.trainable])}\")\n",
    "\n",
    "# Recompile with lower learning rate for fine-tuning3\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_FINE_TUNE),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Model recompiled with learning rate: {LEARNING_RATE_FINE_TUNE}\")\n",
    "print(\"Ready for fine-tuning phase!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc19a49",
   "metadata": {},
   "source": [
    "## Phase 2 Training - Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad446c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 Training - Fine Tuning\n",
    "print(\"--- Starting Training Phase 2: Fine-Tuning ---\")\n",
    "\n",
    "# Continue training with fine-tuning\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS_FEATURE_EXTRACTION + EPOCHS_FINE_TUNE,\n",
    "    initial_epoch=history.epoch[-1] + 1,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"--- Phase 2 Fine-Tuning Complete ---\")\n",
    "print(f\"Completed {len(history_fine.history['accuracy'])} additional epochs\")\n",
    "print(f\"Final training accuracy: {history_fine.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history_fine.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d07b60",
   "metadata": {},
   "source": [
    "## Model Evaluation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a34faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Results\n",
    "print(\"--- Final Model Evaluation ---\")\n",
    "\n",
    "# Load best model\n",
    "best_model = tf.keras.models.load_model('best_palm_model.keras')\n",
    "\n",
    "# Evaluate on validation set\n",
    "loss, accuracy = best_model.evaluate(val_ds, verbose=1)\n",
    "print(f\"\\nFinal Model Performance:\")\n",
    "print(f\"- Validation Loss: {loss:.4f}\")\n",
    "print(f\"- Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"- Total classes: {len(class_names)}\")\n",
    "print(f\"- Training samples: {len(X_train)}\")\n",
    "print(f\"- Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b571e",
   "metadata": {},
   "source": [
    "## Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training History Visualization\n",
    "print(\"--- Plotting Training History ---\")\n",
    "\n",
    "# Combine training histories from both phases\n",
    "acc = history.history['accuracy'] + history_fine.history['accuracy']\n",
    "val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']\n",
    "loss = history.history['loss'] + history_fine.history['loss']\n",
    "val_loss = history.history['val_loss'] + history_fine.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.axvline(len(history.history['accuracy']) - 1, color='gray', linestyle='--', alpha=0.7, label='Start Fine-Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.axvline(len(history.history['loss']) - 1, color='gray', linestyle='--', alpha=0.7, label='Start Fine-Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history visualization complete!\")\n",
    "print(\"Plot saved to: 'training_history.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7be109",
   "metadata": {},
   "source": [
    "## Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7650020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Final Model and Summary\n",
    "print(\"--- Training Complete! ---\")\n",
    "print(\"\\nModel and Results Summary:\")\n",
    "print(f\"✓ Model saved to: 'best_palm_model.keras'\")\n",
    "print(f\"✓ Training history plot saved to: 'training_history.png'\")\n",
    "print(f\"✓ Total classes trained: {len(class_names)}\")\n",
    "print(f\"✓ Total training images: {len(X_train)}\")\n",
    "print(f\"✓ Total validation images: {len(X_val)}\")\n",
    "print(f\"✓ Image size used: {IMG_SIZE}\")\n",
    "print(f\"✓ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"✓ Feature extraction epochs: {EPOCHS_FEATURE_EXTRACTION}\")\n",
    "print(f\"✓ Fine-tuning epochs: {EPOCHS_FINE_TUNE}\")\n",
    "print(f\"✓ Final validation accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nModel is ready for inference!\")\n",
    "print(\"You can load the model using: tf.keras.models.load_model('best_palm_model.keras')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3372f77",
   "metadata": {},
   "source": [
    "## Phase 3 : Prototype Demonstration & Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae40ad1",
   "metadata": {},
   "source": [
    "### Visualize testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de55dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the specific images enrolled for the new users\n",
    "print(\"--- Visualizing Your Enrolled Palm Samples ---\")\n",
    "\n",
    "# --- FIX: Define image_extensions here to make the cell self-contained ---\n",
    "image_extensions = ['*.jpg', '*.JPG', '*.png', '*.PNG', '*.jpeg', '*.JPEG']\n",
    "\n",
    "# Create a list of all user IDs you want to visualize\n",
    "ENROLLED_USER_IDS = ['094', '095'] \n",
    "\n",
    "for user_id in ENROLLED_USER_IDS:\n",
    "    user_dir = data_dir / user_id\n",
    "\n",
    "    if not user_dir.exists():\n",
    "        print(f\"Error: Directory for user '{user_id}' not found.\")\n",
    "        continue \n",
    "    \n",
    "    user_images = []\n",
    "    for ext in image_extensions:\n",
    "        # Use the glob pattern to find files with this extension\n",
    "        user_images.extend(list(user_dir.glob(ext)))\n",
    "    \n",
    "    # Create a new figure for each user's plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Display up to 12 sample images for the current user\n",
    "    for i in range(min(12, len(user_images))):\n",
    "        ax = plt.subplot(3, 4, i + 1)\n",
    "        \n",
    "        img = cv2.imread(str(user_images[i]))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img_rgb)\n",
    "        \n",
    "        plt.title(f\"Palm (ID: {user_id})\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"Training Samples for Enrolled User '{user_id}'\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1f4d5",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc2bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import math\n",
    "\n",
    "print(\"--- Starting Prototype Demonstration ---\")\n",
    "\n",
    "TEST_IMAGE_DIR = 'test_images'\n",
    "CONFIDENCE_THRESHOLD = 0.80 # Lowering threshold slightly for real-world test cases\n",
    "\n",
    "# ### FIX 1: Create a mapping from test folder names to training class IDs ###\n",
    "GROUND_TRUTH_MAP = {\n",
    "    'lyan': '094',\n",
    "    'zidan': '095'\n",
    "}\n",
    "print(f\"Test name mapping: {GROUND_TRUTH_MAP}\")\n",
    "\n",
    "test_dir = pathlib.Path(TEST_IMAGE_DIR)\n",
    "if not test_dir.exists():\n",
    "    print(f\"Error: Test directory '{TEST_IMAGE_DIR}' not found.\")\n",
    "else:\n",
    "    valid_extensions = {'.jpg', '.jpeg', '.png'}\n",
    "    test_image_paths = sorted([f for f in test_dir.rglob('*') if f.suffix.lower() in valid_extensions])\n",
    "    num_test_images = len(test_image_paths)\n",
    "    print(f\"Found {num_test_images} images to test.\")\n",
    "\n",
    "    # Visualization of test images (remains the same)\n",
    "    if num_test_images > 0:\n",
    "        print(\"\\\\n--- Displaying Images to be Tested ---\")\n",
    "        cols = 4\n",
    "        rows = math.ceil(num_test_images / cols)\n",
    "        plt.figure(figsize=(cols * 4, rows * 4))\n",
    "        for i, path in enumerate(test_image_paths):\n",
    "            ax = plt.subplot(rows, cols, i + 1)\n",
    "            img = cv2.imread(str(path))\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f\"Test for: {path.parent.name}\")\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(\"Held-out Test Images\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\\\nLoading the best trained model: 'best_palm_model.keras'\")\n",
    "    final_model = tf.keras.models.load_model('best_palm_model.keras')\n",
    "    print(f\"Model is ready. It recognizes {len(class_names)} users.\")\n",
    "\n",
    "    def run_inference_on_image(image_path, model, class_names_list):\n",
    "        true_user_folder = image_path.parent.name\n",
    "        \n",
    "        # ### FIX 2: Use the map to get the expected training ID ###\n",
    "        if true_user_folder not in GROUND_TRUTH_MAP:\n",
    "            print(f\"\\\\n--- Skipping image from unknown test user: {true_user_folder} ---\")\n",
    "            return\n",
    "        \n",
    "        expected_user_id = GROUND_TRUTH_MAP[true_user_folder]\n",
    "        print(f\"\\\\n--- Testing image: {image_path.name} (True user: {expected_user_id}) ---\")\n",
    "\n",
    "        img = cv2.imread(str(image_path))\n",
    "        roi = extract_palm_roi(img, size=IMG_SIZE)\n",
    "        if roi is None:\n",
    "            print(\"  Result: Could not detect a palm ROI. TEST FAILED.\")\n",
    "            return\n",
    "\n",
    "        img_array = np.expand_dims(roi, axis=0)\n",
    "        predictions = model.predict(img_array, verbose=0)\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "        confidence = np.max(score)\n",
    "        predicted_index = np.argmax(score)\n",
    "        predicted_user = class_names_list[predicted_index]\n",
    "        \n",
    "        print(f\"  Predicted User: '{predicted_user}'\")\n",
    "        print(f\"  Confidence: {confidence:.2%}\")\n",
    "\n",
    "        # ### FIX 3: Compare the prediction against the mapped ID ###\n",
    "        if confidence > CONFIDENCE_THRESHOLD and predicted_user == expected_user_id:\n",
    "            print(\"  Verification Status: CORRECT MATCH ✅\")\n",
    "            result_text = f\"Verified: {predicted_user} ({confidence:.1%})\"\n",
    "            text_color = 'green'\n",
    "        else:\n",
    "            print(\"  Verification Status: INCORRECT MATCH ❌\")\n",
    "            result_text = f\"Predicted {predicted_user} ({confidence:.1%})\"\n",
    "            text_color = 'red'\n",
    "\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(result_text, color=text_color, fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    for path in test_image_paths:\n",
    "        run_inference_on_image(path, final_model, class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
