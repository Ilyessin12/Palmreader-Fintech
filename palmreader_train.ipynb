{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451b557c",
   "metadata": {},
   "source": [
    "# Palm Print Recognition with Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2047582",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be021590",
   "metadata": {},
   "source": [
    "## Setup Environment and GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef214c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Environment and GPU Configuration\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU(s) available: {gpus}\")\n",
    "        print(\"GPU memory growth set.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error setting up GPU: {e}\")\n",
    "else:\n",
    "    print(\"No GPU found, running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee41d9",
   "metadata": {},
   "source": [
    "## Define Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters for model training\n",
    "EPOCHS_FEATURE_EXTRACTION = 10\n",
    "EPOCHS_FINE_TUNE = 5\n",
    "LEARNING_RATE_INITIAL = 0.001\n",
    "LEARNING_RATE_FINE_TUNE = 1e-5\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"- Feature extraction epochs: {EPOCHS_FEATURE_EXTRACTION}\")\n",
    "print(f\"- Fine-tuning epochs: {EPOCHS_FINE_TUNE}\")\n",
    "print(f\"- Image size: {IMG_SIZE}\")\n",
    "print(f\"- Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc21694",
   "metadata": {},
   "source": [
    "## Palm ROI Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_palm_roi(image, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Extracts a square Region of Interest (ROI) from the center of the palm.\n",
    "    This is the most critical step for reliable biometric recognition.\n",
    "    \"\"\"\n",
    "    if image is None:\n",
    "        return None\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contours:\n",
    "        return None\n",
    "\n",
    "    hand_contour = max(contours, key=cv2.contourArea)\n",
    "    M = cv2.moments(hand_contour)\n",
    "    if M[\"m00\"] == 0:\n",
    "        return None\n",
    "    cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "    roi_size = int(min(image.shape[0], image.shape[1]) * 0.4)\n",
    "    half_size = roi_size // 2\n",
    "\n",
    "    x1, y1 = max(0, cx - half_size), max(0, cy - half_size)\n",
    "    x2, y2 = min(image.shape[1], cx + half_size), min(image.shape[0], cy + half_size)\n",
    "\n",
    "    roi = image[y1:y2, x1:x2]\n",
    "    if roi.size == 0:\n",
    "        return None\n",
    "\n",
    "    return cv2.resize(roi, size, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7d9fa5",
   "metadata": {},
   "source": [
    "## Dataset Exploration & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "# Setup dataset path\n",
    "DATASET_DIR = 'datasets/Sapienza University Mobile Palmprint Database(SMPD)'\n",
    "data_dir = pathlib.Path(DATASET_DIR)\n",
    "\n",
    "print(f\"Dataset path set to: {data_dir}\")\n",
    "\n",
    "# Check if dataset exists\n",
    "if not data_dir.exists():\n",
    "    print(f\"Error: Dataset directory not found at '{DATASET_DIR}'\")\n",
    "    print(\"Please ensure the path is correct and the data is unzipped.\")\n",
    "else:\n",
    "    print(\"Dataset directory found!\")\n",
    "\n",
    "# Find all image files\n",
    "image_extensions = ['*.jpg', '*.JPG', '*.png', '*.PNG', '*.jpeg', '*.JPEG']\n",
    "image_paths = []\n",
    "for ext in image_extensions:\n",
    "    image_paths.extend(list(data_dir.glob(f'*/{ext}')))\n",
    "\n",
    "image_count = len(image_paths)\n",
    "print(f\"Total images found: {image_count}\")\n",
    "\n",
    "# Get class names (individual IDs)\n",
    "class_names_list = sorted([item.name for item in data_dir.glob('*') if item.is_dir()])\n",
    "num_classes = len(class_names_list)\n",
    "print(f\"Total classes (individuals): {num_classes}\")\n",
    "\n",
    "print(\"\\nSample of class names (IDs):\", class_names_list[:10])\n",
    "\n",
    "# Show image count per class (sample of first 10 classes)\n",
    "print(\"\\nImage count per class (sample):\")\n",
    "for class_name in class_names_list[:10]:\n",
    "    class_dir = data_dir / class_name\n",
    "    count = 0\n",
    "    for ext in image_extensions:\n",
    "        count += len(list(class_dir.glob(ext)))\n",
    "    print(f\"- ID {class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677658f7",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72fe03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Prepare Dataset with ROI Extraction\n",
    "print(\"Loading and preprocessing dataset with ROI extraction...\")\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "print(f\"Processing {len(class_names_list)} classes...\")\n",
    "\n",
    "for class_idx, class_name in enumerate(class_names_list):\n",
    "    class_dir = data_dir / class_name\n",
    "    class_images = []\n",
    "    \n",
    "    # Get all image files for this class\n",
    "    for ext in image_extensions:\n",
    "        class_images.extend(list(class_dir.glob(ext)))\n",
    "    \n",
    "    print(f\"Processing class {class_name}: {len(class_images)} images\")\n",
    "    \n",
    "    for img_path in class_images:\n",
    "        # Load image\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is not None:\n",
    "            # Extract ROI\n",
    "            roi = extract_palm_roi(img, size=IMG_SIZE)\n",
    "            if roi is not None:\n",
    "                images.append(roi)\n",
    "                labels.append(class_idx)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(images)\n",
    "y = np.array(labels)\n",
    "class_names = class_names_list\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"- Total processed images: {len(X)}\")\n",
    "print(f\"- Image shape: {X.shape}\")\n",
    "print(f\"- Labels shape: {y.shape}\")\n",
    "print(f\"- Number of classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e3040",
   "metadata": {},
   "source": [
    "## Data Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample processed images\n",
    "print(\"--- Visualizing Sample Processed ROI Data ---\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Display first 12 images\n",
    "for i in range(min(12, len(X))):\n",
    "    ax = plt.subplot(3, 4, i + 1)\n",
    "    \n",
    "    # Convert to displayable format\n",
    "    img = X[i].astype(\"uint8\")\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # Get class name from label\n",
    "    label_idx = y[i]\n",
    "    class_name = class_names[label_idx]\n",
    "    plt.title(f\"ID: {class_name}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Sample Palm ROI Images from Dataset\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show class distribution analysis\n",
    "print(\"\\n--- Class Distribution Analysis ---\")\n",
    "unique_labels, counts = np.unique(y, return_counts=True)\n",
    "print(f\"Images per class - Min: {counts.min()}, Max: {counts.max()}, Mean: {counts.mean():.1f}\")\n",
    "\n",
    "# Plot class distribution (for first 20 classes if there are many)\n",
    "plt.figure(figsize=(12, 6))\n",
    "classes_to_show = min(20, len(unique_labels))\n",
    "plt.bar(range(classes_to_show), counts[:classes_to_show])\n",
    "plt.title(f\"Images per Class (First {classes_to_show} classes)\")\n",
    "plt.xlabel(\"Class Index\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xticks(range(classes_to_show), [class_names[i] for i in range(classes_to_show)], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6ef6b",
   "metadata": {},
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"- Training samples: {len(X_train)}\")\n",
    "print(f\"- Validation samples: {len(X_val)}\")\n",
    "print(f\"- Training shape: {X_train.shape}\")\n",
    "print(f\"- Validation shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f469234",
   "metadata": {},
   "source": [
    "## Create TensorFlow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05baa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow datasets with optimization\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_ds = val_ds.batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"TensorFlow datasets created:\")\n",
    "print(f\"- Training batches: {len(train_ds)}\")\n",
    "print(f\"- Validation batches: {len(val_ds)}\")\n",
    "print(f\"- Batch size: {BATCH_SIZE}\")\n",
    "print(f\"- Image size: {IMG_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ee9f2",
   "metadata": {},
   "source": [
    "## Data Augmentation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15825b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation layers for training\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.2),\n",
    "], name='augmentation')\n",
    "\n",
    "print(\"--- Preview of Data Augmentation ---\")\n",
    "\n",
    "# Take one batch and show original vs augmented versions\n",
    "for images, labels in train_ds.take(1):\n",
    "    sample_image = images[0:1]  # Take first image\n",
    "    sample_label = labels[0]\n",
    "    \n",
    "    plt.figure(figsize=(15, 3))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.imshow(sample_image[0].numpy().astype(\"uint8\"))\n",
    "    plt.title(f\"Original\\nID: {class_names[sample_label]}\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Show 4 augmented versions\n",
    "    for i in range(4):\n",
    "        augmented = data_augmentation(sample_image)\n",
    "        plt.subplot(1, 5, i + 2)\n",
    "        plt.imshow(augmented[0].numpy().astype(\"uint8\"))\n",
    "        plt.title(f\"Augmented {i+1}\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(\"Data Augmentation Preview\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "print(\"Data augmentation setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb47b2",
   "metadata": {},
   "source": [
    "## Build Base Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d1689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model with Transfer Learning using MobileNetV2\n",
    "print(\"--- Building Model using Transfer Learning (MobileNetV2) ---\")\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "# Load pre-trained MobileNetV2 base model\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=IMG_SHAPE,\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "base_model._name = 'mobilenetv2_base'\n",
    "base_model.trainable = False  # Freeze base model initially\n",
    "\n",
    "print(f\"Base model loaded: {base_model.name}\")\n",
    "print(f\"Base model has {len(base_model.layers)} layers\")\n",
    "print(f\"Input shape: {IMG_SHAPE}\")\n",
    "print(\"Base model is frozen for initial training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build complete model architecture\n",
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = data_augmentation(x)\n",
    "x = base_model(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(len(class_names), activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "print(\"Model architecture built successfully!\")\n",
    "print(f\"Model will classify {len(class_names)} classes\")\n",
    "print(f\"Final dense layer output: {len(class_names)} neurons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ed50a",
   "metadata": {},
   "source": [
    "## Compile Model for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model for Feature Extraction\n",
    "print(\"--- Compiling the Model ---\")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_INITIAL),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Setup callbacks for training\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_palm_model.keras', \n",
    "        save_best_only=True, \n",
    "        monitor='val_accuracy', \n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5, \n",
    "        monitor='val_loss', \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Model compiled and callbacks configured!\")\n",
    "print(f\"Optimizer: Adam with learning rate {LEARNING_RATE_INITIAL}\")\n",
    "print(\"Loss function: sparse_categorical_crossentropy\")\n",
    "print(\"Metrics: accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4f47e",
   "metadata": {},
   "source": [
    "## Phase 1 Training - Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931582dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 Training - Feature Extraction\n",
    "print(\"--- Starting Training Phase 1: Feature Extraction ---\")\n",
    "print(\"Base model is frozen, training only the classifier head...\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS_FEATURE_EXTRACTION,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"--- Phase 1 Training Complete ---\")\n",
    "print(f\"Completed {len(history.history['accuracy'])} epochs\")\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e604e668",
   "metadata": {},
   "source": [
    "## Model Fine-tuning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5585b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Fine-tuning Setup\n",
    "print(\"--- Preparing for Fine-Tuning ---\")\n",
    "\n",
    "# Unfreeze base model for fine-tuning\n",
    "base_model = model.get_layer('mobilenetv2_base')\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze early layers, only fine-tune top layers\n",
    "fine_tune_at = 100\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"Fine-tuning from layer {fine_tune_at} onwards...\")\n",
    "print(f\"Total layers in base model: {len(base_model.layers)}\")\n",
    "print(f\"Trainable layers: {len([layer for layer in base_model.layers if layer.trainable])}\")\n",
    "\n",
    "# Recompile with lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_FINE_TUNE),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Model recompiled with learning rate: {LEARNING_RATE_FINE_TUNE}\")\n",
    "print(\"Ready for fine-tuning phase!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc19a49",
   "metadata": {},
   "source": [
    "## Phase 2 Training - Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad446c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2 Training - Fine Tuning\n",
    "print(\"--- Starting Training Phase 2: Fine-Tuning ---\")\n",
    "\n",
    "# Continue training with fine-tuning\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS_FEATURE_EXTRACTION + EPOCHS_FINE_TUNE,\n",
    "    initial_epoch=history.epoch[-1] + 1,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"--- Phase 2 Fine-Tuning Complete ---\")\n",
    "print(f\"Completed {len(history_fine.history['accuracy'])} additional epochs\")\n",
    "print(f\"Final training accuracy: {history_fine.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history_fine.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d07b60",
   "metadata": {},
   "source": [
    "## Model Evaluation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a34faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Results\n",
    "print(\"--- Final Model Evaluation ---\")\n",
    "\n",
    "# Load best model\n",
    "best_model = tf.keras.models.load_model('best_palm_model.keras')\n",
    "\n",
    "# Evaluate on validation set\n",
    "loss, accuracy = best_model.evaluate(val_ds, verbose=1)\n",
    "print(f\"\\nFinal Model Performance:\")\n",
    "print(f\"- Validation Loss: {loss:.4f}\")\n",
    "print(f\"- Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"- Total classes: {len(class_names)}\")\n",
    "print(f\"- Training samples: {len(X_train)}\")\n",
    "print(f\"- Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3b571e",
   "metadata": {},
   "source": [
    "## Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training History Visualization\n",
    "print(\"--- Plotting Training History ---\")\n",
    "\n",
    "# Combine training histories from both phases\n",
    "acc = history.history['accuracy'] + history_fine.history['accuracy']\n",
    "val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']\n",
    "loss = history.history['loss'] + history_fine.history['loss']\n",
    "val_loss = history.history['val_loss'] + history_fine.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.axvline(len(history.history['accuracy']) - 1, color='gray', linestyle='--', alpha=0.7, label='Start Fine-Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.axvline(len(history.history['loss']) - 1, color='gray', linestyle='--', alpha=0.7, label='Start Fine-Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history visualization complete!\")\n",
    "print(\"Plot saved to: 'training_history.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7be109",
   "metadata": {},
   "source": [
    "## Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7650020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Final Model and Summary\n",
    "print(\"--- Training Complete! ---\")\n",
    "print(\"\\nModel and Results Summary:\")\n",
    "print(f\"✓ Model saved to: 'best_palm_model.keras'\")\n",
    "print(f\"✓ Training history plot saved to: 'training_history.png'\")\n",
    "print(f\"✓ Total classes trained: {len(class_names)}\")\n",
    "print(f\"✓ Total training images: {len(X_train)}\")\n",
    "print(f\"✓ Total validation images: {len(X_val)}\")\n",
    "print(f\"✓ Image size used: {IMG_SIZE}\")\n",
    "print(f\"✓ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"✓ Feature extraction epochs: {EPOCHS_FEATURE_EXTRACTION}\")\n",
    "print(f\"✓ Fine-tuning epochs: {EPOCHS_FINE_TUNE}\")\n",
    "print(f\"✓ Final validation accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nModel is ready for inference!\")\n",
    "print(\"You can load the model using: tf.keras.models.load_model('best_palm_model.keras')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
